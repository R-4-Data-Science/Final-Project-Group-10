---
title: "Program Function Examples"
author: "Soleil, Sam, Joe"
date: "2025-12-05"
output: html_document
---

This file provides examples for users to understand our code and how to implement it.

## 3.1 Multi-path forward selection

Performs a forward model selection procedure that follows multiple paths 
starting from the intercept-only model and iteratively adding predictors. 
Models that meet the AIC improvement requirement are retained. 
If no model satisfies the requirement, the child model with the lowest AIC is kept.

```{r}

build_paths <- function(
    data,
    response,
    predictors,
    delta = 2,
    eps = 0.5,
    L = 20,
    K = length(predictors),
    verbose = TRUE,
    debug = FALSE
) {
  stopifnot(response %in% colnames(data))
  
  model_key <- function(vars) paste(sort(vars), collapse = "+")
  model_aic <- function(vars) {
    f <- as.formula(paste(response, "~", ifelse(length(vars)==0, "1", paste(vars, collapse="+"))))
    AIC(lm(f, data = data)) #for logical change lm() → glm(..., family = binomial) and ensure response is a factor 
  }
  
  aic_by_model <- list()
  path_forest <- list()
  
  # Step 0: empty model
  empty <- character(0)
  null_aic <- model_aic(empty)
  aic_by_model[[model_key(empty)]] <- null_aic
  path_forest[[1]] <- data.frame(
    model = I(list(empty)),
    AIC = null_aic
  )
  
  step <- 1
  repeat {
    if (step > K) break  # stop if step limit reached
    
    parents <- lapply(path_forest[[step]]$model, unlist)
    if (verbose) message("Step ", step, ": ", length(parents), " parent models")
    
    children_all <- list()
    child_keys <- character()
    
    for (parent in parents) {
      parent_k <- model_key(parent)
      parent_aic <- aic_by_model[[parent_k]]
      
      remaining <- setdiff(predictors, parent)
      if (length(remaining) == 0) next
      
      if (debug) {
        cat("Parent model:", ifelse(length(parent)==0, "(empty)", paste(parent, collapse="+")),
            "AIC =", parent_aic, "\n")
      }
      
      child_info <- lapply(remaining, function(v) {
        vars <- c(parent, v)
        key <- model_key(vars)
        if (is.null(aic_by_model[[key]])) {
          aic_by_model[[key]] <<- model_aic(vars)
        }
        list(vars = vars, key = key, AIC = aic_by_model[[key]])
      })
      
      child_AICs <- sapply(child_info, `[[`, "AIC")
      best_child <- min(child_AICs)
      
      # Keep children within delta and improvement >= eps
      keep_idx <- which((child_AICs - best_child <= delta) &
                          (parent_aic - child_AICs >= eps))
      
      # FORCE keep the best child if none satisfy eps
      if (length(keep_idx) == 0) {
        keep_idx <- which.min(child_AICs)
      }
      
      if (debug) {
        for (i in seq_along(child_info)) {
          vars <- child_info[[i]]$vars
          aic <- child_info[[i]]$AIC
          improvement <- parent_aic - aic
          keep <- ifelse(i %in% keep_idx, "KEEP", "REJECT")
          cat("  Child:", paste(vars, collapse="+"), "AIC =", aic,
              "improvement =", improvement, keep, "\n")
        }
      }
      
      kept <- child_info[keep_idx]
      children_all <- c(children_all, lapply(kept, `[[`, "vars"))
      child_keys <- c(child_keys, sapply(kept, `[[`, "key"))
    }
    
    if (length(child_keys) == 0) {
      if (verbose) message("No children left. Stopping.")
      break
    }
    
    # Deduplicate
    unique_idx <- !duplicated(child_keys)
    frontier <- children_all[unique_idx]
    
    # Prune if > L
    if (length(frontier) > L) {
      AICs <- sapply(frontier, function(v) aic_by_model[[model_key(v)]])
      frontier <- frontier[order(AICs)[1:L]]
    }
    
    # Store frontier
    df_frontier <- data.frame(
      model = I(frontier),
      AIC = sapply(frontier, function(v) aic_by_model[[model_key(v)]])
    )
    path_forest[[step + 1]] <- df_frontier
    
    step <- step + 1
  }
  
  list(
    path_forest = path_forest,
    aic_by_model = aic_by_model,
    meta = list(
      response = response,
      predictors = predictors,
      delta = delta,
      eps = eps,
      L = L,
      K = K
    )
  )
}





#4.1

algorithm_forward_tree <- function(
    X, 
    y, 
    family = c("gaussian", "binomial"),
    K = 5,
    eps = 0.5,
    delta = 2,
    L = 20,
    verbose = TRUE
) {
  
  family <- match.arg(family)
  predictors <- colnames(X)
  
  # Helper: convert to unique model key
  model_key <- function(vars) paste(sort(vars), collapse = "+")
  
  # Helper: fit GLM and compute AIC
  model_aic <- function(vars) {
    df <- data.frame(y = y, X)
    if (length(vars) == 0) {
      f <- as.formula("y ~ 1")
    } else {
      f <- as.formula(paste("y ~", paste(vars, collapse = "+")))
    }
    AIC(glm(f, data = df, family = family))
  }
  
  # Storage
  aic_by_model <- list()
  model_tree   <- list()
  
  # Step 0 — empty model
  empty <- character(0)
  model_tree[[1]] <- list(empty)
  aic_by_model[[model_key(empty)]] <- model_aic(empty)
  
  # --------------------------------------------------------------------------
  # Main Loop
  # --------------------------------------------------------------------------
  
  for (k in 1:K) {
    
    parents <- model_tree[[k]]
    if (verbose) message("Step ", k, ": ", length(parents), " parent models.")
    
    children_all <- list()
    child_keys   <- character()
    
    for (parent in parents) {
      
      parent_key <- model_key(parent)
      parent_aic <- aic_by_model[[parent_key]]
      
      unused <- setdiff(predictors, parent)
      if (length(unused) == 0) next
      
      # Generate child candidates by adding each unused variable
      child_info <- lapply(unused, function(v) {
        vars <- c(parent, v)
        key  <- model_key(vars)
        
        # compute AIC once
        if (is.null(aic_by_model[[key]])) {
          aic_by_model[[key]] <<- model_aic(vars)
        }
        
        list(vars = vars, key = key, AIC = aic_by_model[[key]])
      })
      
      # Evaluate children
      child_AICs <- sapply(child_info, `[[`, "AIC")
      best_child <- min(child_AICs)
      
      # Keep children: must (1) improve parent by eps and (2) be within delta
      keep_idx <- which(
        (parent_aic - child_AICs >= eps) &
          (child_AICs - best_child <= delta)
      )
      
      if (length(keep_idx) > 0) {
        kept <- child_info[keep_idx]
        children_all <- c(children_all, lapply(kept, `[[`, "vars"))
        child_keys   <- c(child_keys, sapply(kept, `[[`, "key"))
      }
    }
    
    # No children → stop early
    if (length(child_keys) == 0) {
      if (verbose) message("Stopping: no AIC-improving children at step ", k, ".")
      break
    }
    
    # Deduplicate
    uniq_index <- !duplicated(child_keys)
    frontier <- children_all[uniq_index]
    
    # Prune if too many
    if (length(frontier) > L) {
      AICs <- sapply(frontier, function(vars) aic_by_model[[model_key(vars)]])
      frontier <- frontier[order(AICs)[1:L]]
      if (verbose) message("Pruning to best ", L, " models.")
    }
    
    model_tree[[k + 1]] <- frontier
  }
  
  return(list(
    models_by_step = model_tree,
    aic_by_model   = aic_by_model,
    meta = list(
      family = family,
      K = K,
      eps = eps,
      delta = delta,
      L = L
    )
  ))
}


```

## 3.2: Resampling with model-set proportions

Uses resampling to quantify how stable each predictor is under the multi-path selection algorithm. The function repeatedly resamples by either bootstrap or subsample techniques and the resulting output is vector with values ranging from 0 to 1. These are stability scores: values closer to 1 indicate predictors that are consistently selected among resamples. Whereas, values closer to 0 indicate predictors that are rarely selected.

```{r}
stability <- function(
    data,
    response,
    predictors,
    B        = 50,
    resample = c("bootstrap", "subsample"),
    m        = NULL,                  
    delta    = 2,
    eps      = 0.5,
    L        = 20,
    K        = length(predictors),
    verbose  = TRUE
) {
  resample <- match.arg(resample)
  n        <- nrow(data)
  p        <- length(predictors)
  
  # accumulator for sum_b z_j^(b)
  pi_sum <- setNames(numeric(p), predictors)
  
  for (b in seq_len(B)) {
    
    ## draw resample indices
    idx <- if (resample == "bootstrap") {
      sample.int(n, n, replace = TRUE)
    } else {
      if (is.null(m)) m <- floor(0.8 * n)
      sample.int(n, m, replace = FALSE)
    }
    
    data_b <- data[idx, , drop = FALSE]
    
    ## run multi-path search 
    paths_b <- build_paths(
      data       = data_b,
      response   = response,
      predictors = predictors,
      delta      = delta,
      eps        = eps,
      L          = L,
      K          = K,
      verbose    = FALSE
    )$path_forest
    
    ## collect all models across steps, drop empty model
    models_b <- unlist(lapply(paths_b, function(df) df$model), recursive = FALSE)
    models_b <- Filter(function(m) length(m) > 0, models_b)
    
    ## z_j^(b) = proportion of models containing predictor j
    z_b <- numeric(p)
    if (length(models_b) > 0) {
      for (j in seq_along(predictors)) {
        v <- predictors[j]
        z_b[j] <- mean(vapply(models_b, function(m) v %in% m, logical(1)))
      }
    }
    
    ## accumulate
    pi_sum <- pi_sum + z_b
    if (verbose) message("Resample ", b, " of ", B)
  }
  
  ## stability vector π_j = (1/B) * sum_b z_j^(b)
  pi_sum / B
}


```

## 3.3: AIC Filter + average stability

plausible_models() combines how well a model fits the data with how stable its predictors are. It keeps only models whose AIC values are within a chosen tolerance of the smallest AIC, making them plausible candidates. For each of these, it then looks at the variables included in the model, averages their stability scores, and discards any model whose average falls below tau.

```{r}
#3.3
plausible_models <- function(path_forest, path_stability, delta=2, tau=0.6) {
  
  AICdf <- data.frame(
    model = names(path_forest$aic_by_model),
    AIC   = unlist(path_forest$aic_by_model),
    vars  = I(strsplit(names(path_forest$aic_by_model), "\\+"))
  )
  
  kept_models_index <- c()
  AICMin <- min(AICdf$AIC)
  
  for (i in 1:length(AICdf$model)) {
    if (AICdf$AIC[i] <= AICMin + delta) {
      kept_models_index <- c(kept_models_index, i)
    }
  }
  
  kept_models <- AICdf[kept_models_index, ]
  
  stability <- NA
  kept_models <- data.frame(kept_models, stability)
  
  remove.idx <- c()
  for (m in 1:length(kept_models$model)) {
    pidx <- which(path_forest$meta$predictors %in% kept_models$vars[[m]])
    
    kept_models$stability[m] <- mean(path_stability[pidx])
    if (kept_models$stability[m] < tau) {
      remove.idx <- c(remove.idx, m * -1)
    }
  }
  
  if (length(remove.idx) >= 1) {
    final_models <- kept_models[remove.idx, ]
  }
  else {
    final_models <- kept_models
  }
  
  return(final_models)
}


```


## 5.1: Linear Regression (Gaussian)
```{r}
set.seed(1)
n <- 120; p <- 8
X <- matrix(rnorm(n*p), n, p)
beta <- c(2, -1.5, 0, 0, 1, rep(0, p-5))
y <- X %*% beta + rnorm(n, sd = 1)
colnames(X) <- paste0("x", 1:p)
df <- as.data.frame(cbind(y, X))

path_forest <- build_paths(data=df, response = "V1", predictors = colnames(X))
path_stability <- stability(data=df, response = "V1", predictors = colnames(X))
plausible_models(path_forest = path_forest, path_stability = path_stability)

```
 

## 5.2 Logistic Regression (Binomial)

```{r}
set.seed(2)
n <- 200; p <- 6
Xb <- matrix(rnorm(n*p), n, p)
linpred <- 1.2*Xb[,1] - 1*Xb[,2] + 0.8*Xb[,5]
prob <- 1 / (1 + exp(-linpred))
ybin <- rbinom(n, 1, prob)
colnames(Xb) <- paste0("x", 1:p)
dfb <- as.data.frame(cbind(y = ybin, Xb))

path_forestb <- build_paths(data=dfb, response = "y", predictors = colnames(Xb))
path_stabilityb <- stability(data=dfb, response = "y", predictors = colnames(Xb))
plausible_models(path_forest = path_forestb, path_stability = path_stabilityb)

```

## 5.3 Example usage 

```{r eval = FALSE}
# install.packages("stepwise")
devtools::install_github("R-4-Data-Science/Final-Project-Group-10/Stepwise")

library(Stepwise)

forest <- build_paths(x = X, y = as.numeric(y), family = "gaussian", K = min(ncol(X), 10), eps = 1e-6, delta = 1, L = 50)

stab <- stability(x = X, y = as.numeric(y), B = 50, resample = "bootstrap", family = "gaussian", K = 10, eps = 1e-6, delta = 1, L = 50)

plaus <- plausible_models(forest, pi = stab$pi, Delta = 2, tau = 0.6)
print(plaus)
```


## References 

3.1, vignette, package setup: https://chatgpt.com/g/g-p-692f0a096ca08191881bf3d46f3d541c-final-project-stats/project

3.2
https://chatgpt.com/share/69333391-c708-8012-b2f8-97e454e422b5

3.3 https://chatgpt.com/c/69319fac-a14c-8328-b66a-87e7044d8fff

